{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71f7c639-44c0-4d2f-a6de-4b0a3d27639a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-lakebridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5cf98be-4270-4db5-b40e-960e884a8555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf8a07cd-e0d6-4bfe-b7c4-2a7402f3222f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "062bcdc6-b296-474a-9a6b-d3032c4d06f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TeradataSparkConverter:\n",
    "    \"\"\"Databricks-optimized Teradata to Spark SQL converter using Lakebridge.\"\"\"\n",
    "    \n",
    "    def __init__(self, catalog: str = \"az_adb_simbus_training\", schema: str = \"td2ss\"):\n",
    "        \"\"\"Initialize converter with Databricks catalog and schema.\"\"\"\n",
    "        self.catalog = catalog\n",
    "        self.schema = schema\n",
    "        self.spark = SparkSession.builder.getOrCreate()\n",
    "        self.conversion_history = []\n",
    "        \n",
    "        # Create tracking table if not exists\n",
    "        self._setup_tracking_table()\n",
    "    \n",
    "    def _setup_tracking_table(self):\n",
    "        \"\"\"Create conversion history tracking table in Unity Catalog.\"\"\"\n",
    "        # self.spark.sql(f\"CREATE CATALOG IF NOT EXISTS {self.catalog}\")\n",
    "        self.spark.sql(f\"USE CATALOG {self.catalog}\")\n",
    "        self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {self.schema}\")\n",
    "        \n",
    "        self.spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.conversion_history (\n",
    "                conversion_id STRING,\n",
    "                timestamp TIMESTAMP,\n",
    "                original_sql STRING,\n",
    "                converted_sql STRING,\n",
    "                status STRING,\n",
    "                warnings ARRAY<STRING>,\n",
    "                fixme_count INT,\n",
    "                conversion_time_ms LONG,\n",
    "                user STRING\n",
    "            ) USING DELTA\n",
    "        \"\"\")\n",
    "    \n",
    "    def convert_single_query(self, teradata_sql: str, \n",
    "                            save_to_catalog: bool = True) -> Dict:\n",
    "        \"\"\"Convert single Teradata query to Spark SQL.\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        conversion_id = f\"conv_{start_time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        # Create temp files\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.sql', delete=False) as f:\n",
    "            f.write(teradata_sql)\n",
    "            input_file = f.name\n",
    "        \n",
    "        output_dir = tempfile.mkdtemp()\n",
    "        print(output_dir)\n",
    "        \n",
    "        try:\n",
    "            # Run Lakebridge conversion\n",
    "            cmd = [\n",
    "                \"databricks\", \"labs\", \"lakebridge\", \"transpile\",\n",
    "                \"--source-dialect\", \"teradata\",\n",
    "                \"--input-source\", input_file,\n",
    "                \"--output-folder\", output_dir,\n",
    "                \"--transpiler\", \"bladebridge\"\n",
    "            ]\n",
    "            \n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)\n",
    "            print(f\"Command: {' '.join(cmd)}\")\n",
    "            print(f\"Return code: {result.returncode}\")\n",
    "            print(f\"STDOUT: {result.stdout}\")\n",
    "            print(f\"ERROR: {result.stderr}\")\n",
    "            \n",
    "            # Process results\n",
    "        #     converted_sql = \"\"\n",
    "        #     warnings = []\n",
    "        #     fixme_count = 0\n",
    "            \n",
    "        #     if result.returncode == 0:\n",
    "        #         # Read converted SQL\n",
    "        #         for sql_file in os.listdir(output_dir):\n",
    "        #             if sql_file.endswith('.sql'):\n",
    "        #                 with open(os.path.join(output_dir, sql_file), 'r') as f:\n",
    "        #                     content = f.read()\n",
    "        #                     converted_sql = content\n",
    "        #                     fixme_count = content.upper().count('FIXME')\n",
    "                            \n",
    "        #                     # Extract warnings\n",
    "        #                     for line in content.split('\\n'):\n",
    "        #                         if 'WARNING' in line.upper() or 'FIXME' in line.upper():\n",
    "        #                             warnings.append(line.strip())\n",
    "                \n",
    "        #         status = \"SUCCESS\"\n",
    "        #     else:\n",
    "        #         status = \"FAILED\"\n",
    "        #         warnings = [result.stderr] if result.stderr else [\"Unknown error\"]\n",
    "        #         print(result.stderr)\n",
    "            \n",
    "        #     # Calculate conversion time\n",
    "        #     conversion_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)\n",
    "            \n",
    "        #     # Prepare result\n",
    "        #     result_dict = {\n",
    "        #         \"conversion_id\": conversion_id,\n",
    "        #         \"timestamp\": start_time,\n",
    "        #         \"original_sql\": teradata_sql[:5000],  # Truncate for storage\n",
    "        #         \"converted_sql\": converted_sql[:5000] if converted_sql else None,\n",
    "        #         \"status\": status,\n",
    "        #         \"warnings\": warnings,\n",
    "        #         \"fixme_count\": fixme_count,\n",
    "        #         \"conversion_time_ms\": conversion_time_ms,\n",
    "        #         \"user\": self.spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "        #     }\n",
    "            \n",
    "        #     # Save to catalog if requested\n",
    "        #     if save_to_catalog and status == \"SUCCESS\":\n",
    "        #         self._save_to_catalog(result_dict)\n",
    "            \n",
    "        #     return result_dict\n",
    "            \n",
    "        finally:\n",
    "            # Cleanup\n",
    "            os.unlink(input_file)\n",
    "            import shutil\n",
    "            shutil.rmtree(output_dir, ignore_errors=True)\n",
    "    \n",
    "    def _save_to_catalog(self, result: Dict):\n",
    "        \"\"\"Save conversion result to Unity Catalog.\"\"\"\n",
    "        df = self.spark.createDataFrame([result])\n",
    "        df.write.mode(\"append\").saveAsTable(\n",
    "            f\"{self.catalog}.{self.schema}.conversion_history\"\n",
    "        )\n",
    "    \n",
    "    def convert_from_table(self, source_table: str, \n",
    "                          query_column: str = \"sql_query\") -> pd.DataFrame:\n",
    "        \"\"\"Convert multiple queries from a Delta table.\"\"\"\n",
    "        queries_df = self.spark.table(source_table).select(query_column).collect()\n",
    "        \n",
    "        results = []\n",
    "        for row in queries_df:\n",
    "            query = row[query_column]\n",
    "            result = self.convert_single_query(query)\n",
    "            results.append(result)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def show_conversion_metrics(self):\n",
    "        \"\"\"Display conversion metrics dashboard.\"\"\"\n",
    "        metrics_df = self.spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_conversions,\n",
    "                SUM(CASE WHEN status = 'SUCCESS' THEN 1 ELSE 0 END) as successful,\n",
    "                SUM(CASE WHEN status = 'FAILED' THEN 1 ELSE 0 END) as failed,\n",
    "                AVG(conversion_time_ms) as avg_time_ms,\n",
    "                SUM(fixme_count) as total_fixme_items,\n",
    "                COUNT(DISTINCT user) as unique_users\n",
    "            FROM {self.catalog}.{self.schema}.conversion_history\n",
    "        \"\"\")\n",
    "        \n",
    "        return metrics_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a05056e2-584f-4d9d-ab10-f300dcffd907",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "converter = TeradataSparkConverter(catalog=\"az_adb_simbus_training\", schema=\"td2ss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2d697aa-da62-48b9-9625-093173679f43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table('az_adb_simbus_training.td2ss.conversion_history').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08155e53-14d7-4b76-9a94-4526191d8e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Convert single Teradata query\n",
    "teradata_query = \"\"\"\n",
    "CREATE SET TABLE sales.monthly_summary ,NO FALLBACK (\n",
    "    month_id INTEGER,\n",
    "    product_id INTEGER,\n",
    "    total_sales DECIMAL(12,2),\n",
    "    units_sold INTEGER\n",
    ") PRIMARY INDEX (month_id, product_id);\n",
    "\n",
    "SELECT \n",
    "    product_id,\n",
    "    SUM(total_sales) as revenue,\n",
    "    COUNT(*) as transactions\n",
    "FROM sales.monthly_summary\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY product_id ORDER BY revenue DESC) <= 10\n",
    "GROUP BY product_id;\n",
    "\"\"\"\n",
    "\n",
    "result = converter.convert_single_query(teradata_query)\n",
    "print(result)\n",
    "\n",
    "# Display result\n",
    "# print(f\"Status: {result['status']}\")\n",
    "# print(f\"Conversion Time: {result['conversion_time_ms']}ms\")\n",
    "# print(f\"FIXME Items: {result['fixme_count']}\")\n",
    "# print(\"\\nConverted SQL:\")\n",
    "# print(result['converted_sql'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e449a2e0-f01d-446f-aa3c-c458827a821c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from databricks_labs_lakebridge import transpile\n",
    "    # or\n",
    "    import lakebridge\n",
    "except ImportError as e:\n",
    "    print(f\"Cannot import: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4366adfc-b3c9-41fb-8fd7-58069c306d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_lakebridge_direct():\n",
    "    \"\"\"Test if we can use lakebridge as a library instead of CLI.\"\"\"\n",
    "    try:\n",
    "        # Check if lakebridge is importable\n",
    "        import databricks_labs_lakebridge\n",
    "        print(\"✅ databricks-labs-lakebridge module found\")\n",
    "        \n",
    "        # Check available functions\n",
    "        print(\"\\nAvailable functions:\")\n",
    "        for item in dir(databricks_labs_lakebridge):\n",
    "            if not item.startswith('_'):\n",
    "                print(f\"  - {item}\")\n",
    "        \n",
    "        return True\n",
    "    except ImportError as e:\n",
    "        print(f\"❌ Cannot import databricks_labs_lakebridge: {e}\")\n",
    "        \n",
    "        # Try alternative import\n",
    "        try:\n",
    "            import lakebridge\n",
    "            print(\"✅ lakebridge module found (alternative import)\")\n",
    "            return True\n",
    "        except ImportError:\n",
    "            print(\"❌ Neither databricks_labs_lakebridge nor lakebridge can be imported\")\n",
    "            return False\n",
    "\n",
    "# Test the import\n",
    "test_lakebridge_direct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "510db622-672b-4dff-984d-e1d0d3435a61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import subprocess\n",
    "\n",
    "print(\"Checking installed packages related to lakebridge:\")\n",
    "for dist in pkg_resources.working_set:\n",
    "    if 'lakebridge' in dist.key.lower():\n",
    "        print(f\"  - {dist.key} version {dist.version} at {dist.location}\")\n",
    "\n",
    "# print(\"\\nChecking pip list:\")\n",
    "# result = subprocess.run([sys.executable, \"-m\", \"pip\", \"list\", \"|\", \"grep\", \"-i\", \"lakebridge\"], \n",
    "#                        capture_output=True, text=True, shell=True)\n",
    "# print(result.stdout)\n",
    "\n",
    "print(\"\\nChecking available CLI commands:\")\n",
    "# Check if databricks CLI is available\n",
    "result = subprocess.run([\"which\", \"databricks\"], capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(f\"  - databricks CLI found at: {result.stdout.strip()}\")\n",
    "    \n",
    "    # Try to run databricks labs list\n",
    "    result = subprocess.run([\"databricks\", \"labs\", \"list\"], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"  - Available labs:\")\n",
    "        print(result.stdout)\n",
    "else:\n",
    "    print(\"  - databricks CLI not found in PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8134f149-dc2b-4082-8a81-f330c9f5484a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subprocess.run([\"databricks\", \"labs\", \"list\"], capture_output=True, text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35cb435c-2b80-46e8-a740-3114e4f1a48e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "teradata_sql = \"\"\"\n",
    "CREATE SET TABLE sales.monthly_summary ,NO FALLBACK (\n",
    "    month_id INTEGER,\n",
    "    product_id INTEGER,\n",
    "    total_sales DECIMAL(12,2),\n",
    "    units_sold INTEGER\n",
    ") PRIMARY INDEX (month_id, product_id);\n",
    "\n",
    "SELECT \n",
    "    product_id,\n",
    "    SUM(total_sales) as revenue,\n",
    "    COUNT(*) as transactions\n",
    "FROM sales.monthly_summary\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY product_id ORDER BY revenue DESC) <= 10\n",
    "GROUP BY product_id;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eedf6bf7-3e74-4cbf-a325-be49ce6f572f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.sql', delete=False) as f:\n",
    "    f.write(teradata_sql)\n",
    "    input_file = f.name\n",
    "\n",
    "output_dir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3209d934-86b7-49fa-beb4-20cff9713000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cmd = [\n",
    "                sys.executable, \"-m\", \"databricks labs lakebridge\", \"transpile\",\n",
    "                \"--source-dialect\", \"teradata\",\n",
    "                \"--input-source\", input_file,\n",
    "                \"--output-folder\", output_dir,\n",
    "                \"--transpiler\", \"bladebridge\"\n",
    "            ]\n",
    "subprocess.run(cmd, capture_output=True, text=True, timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eae72ca-f772-4eb5-9e7c-b5f21b032ab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "converter",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
