{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f4e8904-1455-4a91-8adb-539663edd13e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-lakebridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44cb6d48-1f30-42d3-afb9-d065f59dba65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b3e1d41-292d-4281-a391-213996a82a32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.labs.lakebridge.__about__ import __version__\n",
    "from databricks.labs.lakebridge.config import TranspileConfig, TranspileResult\n",
    "from databricks.labs.lakebridge.transpiler.transpile_engine import TranspileEngine\n",
    "from databricks.labs.lakebridge.transpiler.repository import TranspilerRepository\n",
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"Lakebridge version: {__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f83806f9-a148-491f-9d9f-972a77ec2384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TeradataToSparkConverter:\n",
    "    \"\"\"Lakebridge Python API wrapper for Teradata to Spark conversion.\"\"\"\n",
    "    \n",
    "    def __init__(self, catalog: str = \"az_adb_simbus_training\", schema: str = \"td2ss\"):\n",
    "        self.catalog = catalog\n",
    "        self.schema = schema\n",
    "        self.spark = SparkSession.builder.getOrCreate()\n",
    "        labs_path = \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-7c8c04ee-dc2e-4fe7-a7f0-89823e35d087/lib/python3.11/site-packages\"\n",
    "        \n",
    "        # Initialize transpiler repository\n",
    "        self.repository = TranspilerRepository(labs_path)\n",
    "        \n",
    "        # Setup tracking table\n",
    "        self._setup_tracking_table()\n",
    "    \n",
    "    def _setup_tracking_table(self):\n",
    "        \"\"\"Create conversion history tracking table.\"\"\"\n",
    "        self.spark.sql(f\"USE CATALOG {self.catalog}\")\n",
    "        self.spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {self.schema}\")\n",
    "        \n",
    "        self.spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.conversion_history (\n",
    "                conversion_id STRING,\n",
    "                timestamp TIMESTAMP,\n",
    "                original_sql STRING,\n",
    "                converted_sql STRING,\n",
    "                status STRING,\n",
    "                error_message STRING,\n",
    "                conversion_time_ms LONG,\n",
    "                user STRING\n",
    "            ) USING DELTA\n",
    "        \"\"\")\n",
    "    \n",
    "    def convert_sql(self, teradata_sql: str, save_to_catalog: bool = True):\n",
    "        \"\"\"Convert Teradata SQL to Spark SQL using Lakebridge Python API.\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        conversion_id = f\"conv_{start_time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        # Create temporary directory for input/output\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            input_file = os.path.join(temp_dir, \"input.sql\")\n",
    "            output_dir = os.path.join(temp_dir, \"output\")\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Write input SQL to file\n",
    "            with open(input_file, 'w') as f:\n",
    "                f.write(teradata_sql)\n",
    "            \n",
    "            try:\n",
    "                # Create transpile configuration\n",
    "                config = TranspileConfig(\n",
    "                    source_dialect=\"teradata\",\n",
    "                    # target_dialect=\"spark\",\n",
    "                    input_source=input_file,\n",
    "                    output_folder=output_dir,\n",
    "                    # transpiler=\"bladebridge\"  # Use BladeBridge engine\n",
    "                )\n",
    "                \n",
    "                # Create and run transpile engine\n",
    "                engine = TranspileEngine()\n",
    "                result = engine.transpile(config)\n",
    "                print(result)\n",
    "                \n",
    "                # Read converted SQL from output\n",
    "                converted_sql = None\n",
    "                output_files = list(Path(output_dir).glob(\"*.sql\"))\n",
    "                if output_files:\n",
    "                    with open(output_files[0], 'r') as f:\n",
    "                        converted_sql = f.read()\n",
    "                \n",
    "                status = \"SUCCESS\" if converted_sql else \"FAILED\"\n",
    "                error_message = None if converted_sql else \"No output generated\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                converted_sql = None\n",
    "                status = \"FAILED\"\n",
    "                error_message = str(e)\n",
    "                print(f\"Conversion error: {e}\")\n",
    "        \n",
    "        # Calculate conversion time\n",
    "        conversion_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)\n",
    "        \n",
    "        # Prepare result\n",
    "        result_dict = {\n",
    "            \"conversion_id\": conversion_id,\n",
    "            \"timestamp\": start_time,\n",
    "            \"original_sql\": teradata_sql[:5000],\n",
    "            \"converted_sql\": converted_sql[:5000] if converted_sql else None,\n",
    "            \"status\": status,\n",
    "            \"error_message\": error_message,\n",
    "            \"conversion_time_ms\": conversion_time_ms,\n",
    "            \"user\": self.spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "        }\n",
    "        \n",
    "        # Save to catalog\n",
    "        if save_to_catalog:\n",
    "            df = self.spark.createDataFrame([result_dict])\n",
    "            df.write.mode(\"append\").saveAsTable(\n",
    "                f\"{self.catalog}.{self.schema}.conversion_history\"\n",
    "            )\n",
    "        \n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca31542d-1aeb-4a4a-a359-b43684ed221e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize converter\n",
    "converter = TeradataToSparkConverter(catalog=\"az_adb_simbus_training\", schema=\"td2ss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "733a1902-527a-4796-8c3b-792b6710488b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test conversion\n",
    "teradata_query = \"\"\"\n",
    "CREATE SET TABLE sales.monthly_summary ,NO FALLBACK (\n",
    "    month_id INTEGER,\n",
    "    product_id INTEGER,\n",
    "    total_sales DECIMAL(12,2),\n",
    "    units_sold INTEGER\n",
    ") PRIMARY INDEX (month_id, product_id);\n",
    "\n",
    "SELECT \n",
    "    product_id,\n",
    "    SUM(total_sales) as revenue,\n",
    "    COUNT(*) as transactions\n",
    "FROM sales.monthly_summary\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY product_id ORDER BY revenue DESC) <= 10\n",
    "GROUP BY product_id;\n",
    "\"\"\"\n",
    "\n",
    "# Try the converter\n",
    "result = converter.convert_sql(teradata_query)\n",
    "\n",
    "print(f\"Status: {result['status']}\")\n",
    "print(f\"Conversion Time: {result['conversion_time_ms']}ms\")\n",
    "if result['converted_sql']:\n",
    "    print(\"\\nConverted SQL:\")\n",
    "    print(result['converted_sql'])\n",
    "else:\n",
    "    print(f\"\\nError: {result['error_message']}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# If the above doesn't work, try the direct approach\n",
    "print(\"Trying direct conversion...\")\n",
    "converted = convert_teradata_to_spark_direct(teradata_query)\n",
    "if converted:\n",
    "    print(\"Direct conversion successful:\")\n",
    "    print(converted)\n",
    "else:\n",
    "    print(\"Direct conversion failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d37b940f-2fea-4fca-a513-541e25e6630b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "teradata_query = \"\"\"\n",
    "CREATE SET TABLE sales.monthly_summary ,NO FALLBACK (\n",
    "    month_id INTEGER,\n",
    "    product_id INTEGER,\n",
    "    total_sales DECIMAL(12,2),\n",
    "    units_sold INTEGER\n",
    ") PRIMARY INDEX (month_id, product_id);\n",
    "\n",
    "SELECT \n",
    "    product_id,\n",
    "    SUM(total_sales) as revenue,\n",
    "    COUNT(*) as transactions\n",
    "FROM sales.monthly_summary\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY product_id ORDER BY revenue DESC) <= 10\n",
    "GROUP BY product_id;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1a29412-1b30-4ce7-a185-3551badc0de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "convert_teradata_to_spark_direct(teradata_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d46576a-f7c3-48d2-8b98-3c7d0bdec121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Troubleshooting\n",
    "# MAGIC \n",
    "# MAGIC If the above approaches don't work, the Python API may not be fully exposed. \n",
    "# MAGIC You can try:\n",
    "# MAGIC \n",
    "# MAGIC 1. **Inspect available methods:**\n",
    "# MAGIC ```python\n",
    "# MAGIC from databricks.labs.lakebridge.transpiler.transpile_engine import TranspileEngine\n",
    "# MAGIC print(dir(TranspileEngine))\n",
    "# MAGIC ```\n",
    "# MAGIC \n",
    "# MAGIC 2. **Check TranspileConfig attributes:**\n",
    "# MAGIC ```python\n",
    "# MAGIC from databricks.labs.lakebridge.config import TranspileConfig\n",
    "# MAGIC config = TranspileConfig()\n",
    "# MAGIC print(dir(config))\n",
    "# MAGIC ```\n",
    "# MAGIC \n",
    "# MAGIC 3. **Use SQLGlot instead** (shown in previous response) which has a well-documented Python API"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "converterV2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
